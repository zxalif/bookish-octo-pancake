# Docker Compose for ClientHunt Backend (lead-api) - PRODUCTION
# Uses native PostgreSQL and Redis (not Docker containers)
# Port: 7300
#
# IMPORTANT: This file is for production deployment on VPS
# - Uses host network mode (localhost works directly)
# - Uses native PostgreSQL on host (via localhost)
# - Uses native Redis on host (via localhost)
# - No source code mounting (production build)
# - Resource limits configured
#
# NETWORK MODE: host
# - Container shares host's network stack
# - localhost in container = host's localhost
# - Simpler configuration, no host.docker.internal needed
# - No port mapping needed (uses host ports directly)

services:
  # ===== Backend API Service =====
  lead-api:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: clienthunt-api
    network_mode: host  # Use host network - localhost works directly!
    environment:
      # Database (Native PostgreSQL on host)
      - DATABASE_URL=${DATABASE_URL}
      
      # JWT Authentication
      - SECRET_KEY=${SECRET_KEY}
      - ALGORITHM=HS256
      - ACCESS_TOKEN_EXPIRE_MINUTES=1440
      
      # Paddle Payment Gateway
      - PADDLE_ENABLED=${PADDLE_ENABLED:-false}
      - PADDLE_API_KEY=${PADDLE_API_KEY}
      - PADDLE_VENDOR_ID=${PADDLE_VENDOR_ID}
      - PADDLE_ENVIRONMENT=${PADDLE_ENVIRONMENT:-live}
      - PADDLE_WEBHOOK_SECRET=${PADDLE_WEBHOOK_SECRET}
      
      # Rixly API Integration
      # localhost works because we're using host network mode!
      - RIXLY_API_URL=${RIXLY_API_URL:-http://localhost:8000}
      - RIXLY_API_KEY=${RIXLY_API_KEY}
      
      # Email Configuration
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USER=${SMTP_USER}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SMTP_FROM_EMAIL=${SMTP_FROM_EMAIL}
      - SMTP_FROM_NAME=${SMTP_FROM_NAME:-ClientHunt}
      - SMTP_NOREPLY_EMAIL=${SMTP_NOREPLY_EMAIL}
      - SMTP_WELCOME_EMAIL=${SMTP_WELCOME_EMAIL}
      
      # API Configuration
      - API_URL=${API_URL:-https://api.clienthunt.app}
      - FRONTEND_URL=${FRONTEND_URL:-https://clienthunt.app}
      
      # CORS Configuration
      - CORS_ORIGINS=${CORS_ORIGINS:-https://clienthunt.app,https://www.clienthunt.app,https://admin.clienthunt.app}
      
      # Redis Cache (Native Redis on host)
      - REDIS_URL=${REDIS_URL}
      
      # Application Settings
      - APP_NAME=ClientHunt
      - DEBUG=False
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=/app/logs/app.log
      
      # Monitoring (Sentry)
      - SENTRY_DSN=${SENTRY_DSN}
      - SENTRY_ENABLED=${SENTRY_ENABLED:-true}
      
      # Auto-run migrations on startup
      - AUTO_RUN_MIGRATIONS=${AUTO_RUN_MIGRATIONS:-true}
      
      # Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    volumes:
      # Logs volume
      - api_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7300/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  # ===== Scheduler Service (Cron Jobs) =====
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile.scheduler
    container_name: clienthunt-scheduler
    network_mode: host  # Use host network - same as API
    environment:
      # Database (Native PostgreSQL on host)
      - DATABASE_URL=${DATABASE_URL}
      
      # Paddle Payment Gateway
      - PADDLE_ENABLED=${PADDLE_ENABLED:-true}
      - PADDLE_API_KEY=${PADDLE_API_KEY}
      - PADDLE_VENDOR_ID=${PADDLE_VENDOR_ID}
      - PADDLE_ENVIRONMENT=${PADDLE_ENVIRONMENT:-live}
      - PADDLE_WEBHOOK_SECRET=${PADDLE_WEBHOOK_SECRET}
      
      # Application Settings
      - APP_NAME=ClientHunt
      - DEBUG=False
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=/app/logs/scheduler.log
      # Auto-run migrations on startup (if API doesn't run them)
      # In production, scheduler waits for API to complete migrations
      - AUTO_RUN_MIGRATIONS=${AUTO_RUN_MIGRATIONS:-false}
      # Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    volumes:
      # Logs volume
      - scheduler_logs:/app/logs
    # Wait for API to complete migrations before starting scheduler
    # Note: In host network mode, we can't use depends_on with healthcheck
    # Instead, scheduler will wait for database to be ready and migrations to complete
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "-f", "scheduler.py"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

  # ===== E2E Test Worker Service =====
  e2e-worker:
    build:
      context: .
      dockerfile: Dockerfile.e2e-worker
    container_name: clienthunt-e2e-worker
    network_mode: host  # Use host network - same as API
    environment:
      # Database (Native PostgreSQL on host)
      - DATABASE_URL=${DATABASE_URL}
      
      # Redis Cache (Native Redis on host, for job queue)
      - REDIS_URL=${REDIS_URL}
      
      # API Configuration (for test execution)
      # Production: Use actual domain URLs
      - API_URL=${API_URL:-https://api.clienthunt.app}
      - FRONTEND_URL=${FRONTEND_URL:-https://clienthunt.app}
      
      # Application Settings
      - APP_NAME=ClientHunt
      - DEBUG=False
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=/app/logs/e2e-worker.log
      # Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Playwright settings
      - PLAYWRIGHT_BROWSERS_PATH=/app/.cache/ms-playwright
      # Security: Disable watch mode in production
      - WATCH_MODE=false
    volumes:
      # Logs volume (read-write for logs)
      - e2e_worker_logs:/app/logs
      # Persist Playwright browser cache (read-write for browser cache)
      - playwright_browsers:/app/.cache/ms-playwright
    # Security: Run as non-root user (created in Dockerfile)
    user: "1000:1000"
    # Security: Read-only root filesystem (except volumes)
    read_only: true
    # Security: Temporary filesystem for /tmp
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    restart: unless-stopped
    healthcheck:
      # Check if worker process is running (works with non-root user)
      test: ["CMD", "pgrep", "-f", "run_e2e_worker.py"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s  # Give time for Playwright installation
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G  # More memory for Playwright browsers
        reservations:
          cpus: '1'
          memory: 2G

# ===== Volumes =====
# NOTE: No networks section needed with host network mode
volumes:
  api_logs:
    driver: local
    name: clienthunt-api-logs
  scheduler_logs:
    driver: local
    name: clienthunt-scheduler-logs
  e2e_worker_logs:
    driver: local
    name: clienthunt-e2e-worker-logs
  playwright_browsers:
    driver: local
    name: clienthunt-playwright-browsers

